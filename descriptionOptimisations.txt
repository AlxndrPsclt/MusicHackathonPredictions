La mesure de la performance requise par le concours étaient la Root Mean Square Error. Une fois un premier réseau naïf mais fonctionnel a été mis en place, plusieurs essais ont été menés pour tenter d'améliorer les perfomances de ce modele.
Pour améliorer les performances du réseaux neuronal, plusieurs configurations de couches de neurones ont été testés pour sélectionner les meilleures.
La pratique courante est d'utiliser une seule couche cachée contenant (nombre d'entrées + nombre de sorties)/2 neurones. Cela a cependant présenté pour nous un problme technique : les données que nous utilisons contiennent 444 entrées et plus de 100 000 éléments. Charger ces données en mémoire occupaient une si grande partie de la ram que l'entainement du réseau neuronal dans la configuration recommandée régulièrement par des dépassements de mémoire. De plus, ayant transformés certaiens entreés comme le numéro de l'artiste en vecteur binaire à 44 dimensions le dataset ne peux plus être considéré comme contenant 444 items, certains de ces items étant corrélés.
Les configurations testées contenaient donc entre 1 et 3 couches de neurones (sans compter la couche de sortie). Chaque couche contient un nombre de neurone inférieur à la couche précédente. Dans un premier temps, l’entraînement se faisait avec 10 cycles de Backpropagation, pour des questions de rapidité de calcul. La learning rate par défaut de 0.2 a été conservée. Les données étaient toutes normalisées par la méthode du MinMax.
Les donnés en entrées présentent 444 items, les premières couches testées possédaient entre 10 et 100 neurones. Le temps d’entraînement au-delà de 100 neurones était prohibitif et ces configurations n'ont pas été explorées. Il a également rapidement été constaté qu'aller au delà de 100 neurones n'apportait aucun gain mais rallongait drastiquement le temps de calcul.
Une seconde couche a été rajoutée, contenant entre 5 et 100 neurones, avec la contrainte de posséder au plus 2 fois moins de neurones que la couche précédente (c'est à dire que la configuration Entrée->80 neurones->20 neurones->Sortie a été acceptée mais pas la configuration Entrée->80 neurones->60 neurones->Sortie). Le modèle est devenu cette fois ci raisonablement performant dans toutes les configurations sauf encore une fois celles contenant un nombre trop grand de neurones au total (Nombre total de neurones > 100). Dans ces cas l'entraînement était très long sans aboutir à des améliorations significatives du modèle.
La troisième et dernière couche testée contenait entre 2 et 50 neurones avec la même contrainte que pour la deuxième couche.
Toutes les couches utilisent une sigmoide comme fonction d'activation.
Le modèle le plus performant a été celui contenant 22 neurones dans la première couche et 10 neurones dans la seconde (pas de troisième couche).
Un travail a ensuite été réalisé pour déterminer le nombre optimal de cycles d'entrainement. Pour éviter l'apprentissage par coeur, la technique de cross-validation (entrainement sur 90% des données, test sur 10%, avec partage aléatoire pour chacune des 10 étapes de crossvalidation) a été utilisée et la valeur moyenne du RMSE a été calculée.
Le meilleur modèle obtenu est :
